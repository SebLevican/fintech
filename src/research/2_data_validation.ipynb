{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Optional\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataValidationArtifact:\n",
    "    validation_status: bool\n",
    "    drift_report_file_path: Optional\n",
    "    dataframes: Dict[str,pd.DataFrame]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\roser\\\\OneDrive\\\\Documentos\\\\fint'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import os\n",
    "from src.constants import training_pipeline\n",
    "\n",
    "\n",
    "class TrainingPipelineConfig:\n",
    "    def __init__(self,timestamp=datetime.now()):\n",
    "        timestamp =timestamp.strftime('%m_%d_%Y_%H_%M_S')\n",
    "        self.pipeline_name=training_pipeline.PIPELINE_NAME\n",
    "        self.artifact_name=training_pipeline.ARTIFACT_DIR\n",
    "        self.artifact_dir=os.path.join(self.artifact_name,timestamp)\n",
    "        self.model_dir=os.path.join('final_model')\n",
    "        self.timestamp: str=timestamp\n",
    "\n",
    "class DataValidationConfig:\n",
    "    def __init__(self,training_pipeline_config:TrainingPipelineConfig):\n",
    "        self.data_validation_dir: str=os.path.join(training_pipeline_config.artifact_dir, training_pipeline.DATA_VALIDATION_DIR_NAME)\n",
    "        self.valid_data_dir:str= os.path.join(self.data_validation_dir, training_pipeline.DATA_VALIDATION_VALID_DIR)\n",
    "        self.invalid_data_dir:str= os.path.join(self.data_validation_dir,training_pipeline.DATA_VALIDATION_INVALID_DIR)\n",
    "        self.drift_report_file_path:str =os.path.join(self.data_validation_dir, training_pipeline.DATA_VALIDATION_DRIFT_REPORT_DIR,training_pipeline.DATA_VALIDATION_DRIFT_REPORT_FILE_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.exception.exception import FintechException\n",
    "from src.logging.logger import logging\n",
    "from src.constants.training_pipeline import SCHEMA_FILE_PATH\n",
    "from src.utils.common import read_yaml_file,write_yaml_file\n",
    "from src.entity.artifact_entity import DataIngestionArtifact\n",
    "from src.entity.config_entity import DataIngestionConfig\n",
    "import pandas as pd\n",
    "import sys\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataValidation:\n",
    "    def __init__(self,data_ingestion_artifact:DataIngestionArtifact,\n",
    "                 data_validation_config:DataValidationConfig):\n",
    "        try:\n",
    "            self.data_ingestion_artifact=data_ingestion_artifact\n",
    "            self.data_validation_config=data_validation_config\n",
    "            self._schema_config= read_yaml_file(SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise FintechException(e,sys)\n",
    "        \n",
    "    @staticmethod\n",
    "    def read_data(file_path)->pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise FintechException\n",
    "        \n",
    "    def validate_number_of_columns(self,df,dataframe:pd.DataFrame)->bool:\n",
    "        try:\n",
    "            number_of_columns=len(self._schema_config[df]['columns'])\n",
    "            logging.info(f'Required number of columns: {number_of_columns}')\n",
    "            logging.info(f'Data frame has columns:{len(dataframe.columns)}')\n",
    "            if len(dataframe.columns) == number_of_columns:\n",
    "                return True\n",
    "        except Exception as e:\n",
    "            raise FintechException(e,sys)\n",
    "        \n",
    "    def detect_dataset_drift(self,base_df,current_df,threshold=0.5)->bool:\n",
    "        try:\n",
    "            status=True\n",
    "            report={}\n",
    "            for column in base_df:\n",
    "                d1=base_df[column]\n",
    "                d2=current_df[column]\n",
    "                is_same_dist=ks_2samp(d1,d2)\n",
    "                if threshold <= is_same_dist.pvalue:\n",
    "                    is_found=False\n",
    "                else:\n",
    "                    is_found=True\n",
    "                    status=False\n",
    "                report.update({column:{\n",
    "                    'p_value':float(is_same_dist.pvalue),\n",
    "                    'drift_status':is_found\n",
    "                }})\n",
    "            drift_report_file_path=self.data_validation_config.drift_report_file_path\n",
    "\n",
    "            dir_path = os.path.dirname(drift_report_file_path)\n",
    "            os.makedirs(dir_path,exist_ok=True)\n",
    "            write_yaml_file(filep_path=drift_report_file_path,content=report)\n",
    "        except Exception as e:\n",
    "            raise FintechException(e,sys)\n",
    "        \n",
    "    def clean_columns(self, df:pd.DataFrame)-> pd.DataFrame:\n",
    "        try:\n",
    "\n",
    "            columns_transformations ={\n",
    "            \"credit_limit\": lambda x: x.str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float).fillna(0).astype(int),\n",
    "            \"per_capita_income\": lambda x: x.str.replace(\"$\", \"\").str.replace(\",\", \"\").str.replace(\"50K\", \"50000\").fillna(\"0\"),\n",
    "            \"yearly_income\": lambda x: x.str.replace(\"$\", \"\").astype(float).fillna(0).astype(int),\n",
    "            \"total_debt\": lambda x: x.str.replace(\"$\", \"\").astype(float).fillna(0).astype(int),\n",
    "            \"amount\": lambda x: x.str.replace(\"$\", \"\").str.replace(\",\", \"\").astype(float).fillna(0).astype(int)\n",
    "            }\n",
    "\n",
    "            for col, func in columns_transformations.items():\n",
    "                if col in df.columns:\n",
    "                    df[col] = func(df[col])\n",
    "\n",
    "            if 'credit_limit' in df.columns:\n",
    "                df = df.dropna(subset=['credit_limit'])\n",
    "\n",
    "            if 'amount' in df.columns:\n",
    "                df = df.dropna(subset=['amount'])\n",
    "\n",
    "            return df\n",
    "        \n",
    "        except Exception as e:\n",
    "            raise FintechException(e,sys)\n",
    "\n",
    "\n",
    "\n",
    "    def initiate_data_validation(self,df_name: str, df: pd.DataFrame) -> DataValidationArtifact:\n",
    "        try:\n",
    "            df= self.clean_columns(df)\n",
    "\n",
    "            status = self.validate_number_of_columns(df_name,df)\n",
    "\n",
    "            if status:\n",
    "                logging.info(f'Validation columns sucessful')\n",
    "            else:\n",
    "                logging.warning(f'Validation columns error for {df_name}')\n",
    "\n",
    "            data_validation_artifact = DataValidationArtifact(\n",
    "                validation_status = status,\n",
    "                drift_report_file_path = self.data_validation_config.drift_report_file_path,\n",
    "                dataframes = df\n",
    "            )\n",
    "            return data_validation_artifact\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            raise FintechException(e,sys)\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
